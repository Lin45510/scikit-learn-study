{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9533c9ab",
   "metadata": {},
   "source": [
    "[<- Anterior](encoding.ipynb) | [Próximo ->](../validacao/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418f2ae7",
   "metadata": {},
   "source": [
    "# Pipelines e ColumnTransformers\n",
    "\n",
    "Pipelines e ColumnTransformers são estruturas que permitem padronizar o pré-processamento de dados e o treinamento de um modelo dentro de um único objeto, tornando a etapa de pré-processamento mais simples.\n",
    "\n",
    "**O que será abordado**\n",
    "- Imports\n",
    "- Dataset de teste\n",
    "- ColumnTransformers\n",
    "- Pipelines\n",
    "- Exemplo de uso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a388e07",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9eaef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import da biblioteca Pandas -> Utilizada na visualização dos dados e criação do Dataframe\n",
    "import pandas as pd\n",
    "\n",
    "# Importação da Classe SimpleImputer -> Utilizada nas imputações de valores nulos ou ausentes\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Importação da Classe OneHotEncoder -> Utilizada para realizar a categorização dos dados\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Importação da função train_test_split -> Utilizada para divisão do dataset em conjunto de treino e conjunto de teste\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Importa a classe LogisticRegression -> Modelo de Classificação que será usado na sessão de exemplo de aplicação\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81662b70",
   "metadata": {},
   "source": [
    "# Dataset de teste\n",
    "\n",
    "Para o estudo do processo de criação de pipelines será utilizado um dataset que possui diferentes tipos de dados e valores nulos. Dessa forma o dataframe de teste pode se aproximar de um caso real e pode ser usado para estudar diferentes ferramentas de pré-processamento.\n",
    "\n",
    "O dataset utilizado está disponível na pasta Data desse diretório ([aperte aqui para acessar](../Data/PreProcessing_Test.csv)) e possui 100 linhas, apresentando valores nulos nas colunas 'Genero' e 'Salario'. O dataset apresenta uma coluna binaria 'Comprou' que indica se uma pessoa comprou um certo produto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac8efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#leitura do dataset\n",
    "df = pd.read_csv('../Data/pre_processing_test.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ca1d6",
   "metadata": {},
   "source": [
    "# ColumnTransformers\n",
    "\n",
    "ColumnTransformers são objetos que indicam a ordem e quais procedimentos que devem ser realizados em um ou mais conjuntos de dados.\n",
    "\n",
    "Para criar um column transformer é necessário criar um objeto da classe ColumnTransformer, que recebe como parâmetro um array contendo tuplas que descrevem os procedimentos que serão realizados por ele. Exemplo:   \n",
    "\n",
    "```py\n",
    "nome_do_ColumnTransformer = ColumnTransformer([   \n",
    "    (nome_do_processo, função_a_ser_realizada, conjundo_de_dados_que_será_modificado)   \n",
    "    (nome_do_processo2, função_a_ser_realizada2, conjundo_de_dados_que_será_modificado) #Opcional   \n",
    "])\n",
    "```\n",
    "\n",
    "Veja um exemplo abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff64444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imprta a Classe ColumnTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#ColumnTransformer para as colunas Numericas\n",
    "num_transformer = ColumnTransformer([\n",
    "    ('num_imputer', SimpleImputer(strategy='mean'), ['Idade', 'Salario']), # Imputer das colunas numericas\n",
    "    ('scaler', StandardScaler(), ['Idade', 'Salario']) # Scaler das colunas numericas\n",
    "])\n",
    "\n",
    "#ColumnTransformer para as colunas de Texto\n",
    "str_transformer = ColumnTransformer([\n",
    "    ('Imputer', SimpleImputer(strategy='most_frequent'), df[['Genero', 'Setor']]),\n",
    "    ('cat', OneHotEncoder(sparse_output=False), ['Genero', 'Setor'])\n",
    "])\n",
    "\n",
    "#Alplica o ColumnTransform numerico ao dataframe \n",
    "num_cols = num_transformer.fit_transform(df)\n",
    "\n",
    "#transformação do resultado em um dataframe: ESSA ETAPA SERVE APENAS PARA VISUALIZAR OS REUSLTADOS\n",
    "num_cols_df = pd.DataFrame(num_cols, columns=num_transformer.get_feature_names_out())\n",
    "num_cols_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdccb39",
   "metadata": {},
   "source": [
    "Perceba que, ao aplicar o transformer numérico ao dataframe o resultado é um Array que contém os dados da aplicação de cada etapa nas colunas desejadas. Exemplo:\n",
    "\n",
    "- A primeira etapa do ColumnTansformer criado é a aplicação do `SimpleImputer()` nas colunas 'Idade' e 'Salario'. Portanto as primeiras colunas trazem o resultado dessa função, sendo que, cada uma representa o resultado em uma coluna diferente.\n",
    "- A segunda etapa do ColumnTransformer criado é a aplicação do `StandarScaler()` nas colunas 'Idade' e 'Salario'. Portanto as colunas colunas trazem o resultado dessa função, sendo que, cada uma representa o resultado em uma coluna diferente.\n",
    "\n",
    "Perceba que, ao realizar o `.fit_tranform()` em um dataframe do Pandas não é necessário informar quais colunas devem sofrer a aplicação das funções. Isso acontece por que o nome das colunas no **ColumnTransform** utilizado possuem os mesmos nomes das colunas no dataframe em que a função deve ser aplicada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95144b5",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "Pipelines são objetos que indicam em qual sequencia um conjunto de procedimentos deve ser realizado.\n",
    "\n",
    "Para criar um pipeline é necessário criar um objeto da classe Pipeline, que recebe como parâmetro um array contendo tuplas que descrevem os procedimentos que serão realizados por ele. Exemplo:   \n",
    "\n",
    "```py\n",
    "nome_do_Pipeline = Pipeline([    \n",
    "    (nome_do_processo, função_a_ser_realizada)    \n",
    "    (nome_do_processo2, função_a_ser_realizada2) #Opcional    \n",
    "])\n",
    "```\n",
    "\n",
    "Veja um exemplo abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ad14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importação da Classe de Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Criação do Pipeline numérico\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# do pipeline de Texto\n",
    "str_pipeline = num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('cat', OneHotEncoder(sparse_output=False))\n",
    "])\n",
    "\n",
    "#Aplica o Pipeline de texto ao dataframe \n",
    "str_cols = str_pipeline.fit_transform(df[['Genero', 'Setor']])\n",
    "\n",
    "#transformação do resultado em um dataframe: ESSA ETAPA SERVE APENAS PARA VISUALIZAR OS REUSLTADOS\n",
    "str_cols_df = pd.DataFrame(str_cols)\n",
    "str_cols_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1ecbb5",
   "metadata": {},
   "source": [
    "Perceba que, ao aplicar o pipeline de Texto às colunas 'Genero' e 'Setor' o resultado é um Array que comtém os dados da aplicação de cada etapa nas colunas desejadas. Exemplo:\n",
    "\n",
    "- A primeira etapa do Pipeline criado é a aplicação do `SimpleImputer()` nos dados fornecidos como parametro do método `.fit_transform()`. Portanto as primeiras colunas trazem o resultado dessa função, sendo que, cada uma representa o resultado em uma coluna diferente.\n",
    "- A segunda etapa do Pipeline criado é a aplicação do `OneHotEncoder()` nos dados fornecidos como parametro do método `.fit_transform()`. Portanto as colunas trazem o resultado dessa função, sendo que, cada uma representa o resultado em uma coluna diferente.\n",
    "\n",
    "Perceba que, ao realizar o `.fit_tranform()` é necessário informar qual conjunto de dados deve receber as funções informadas dentro do pipeline. Isso acontece por que, ao contrário do que ocorre com a classe [ColumnTransform](#columntransformers), pipelines apenas aplicam as funções informadas, sem saber de fato à quais colunas esses processos devem ser aplicados.\n",
    "\n",
    "**ESSE CASO É APENAS UM EXEMPLO PARA ESCLARECER O FUNCIONAMENTO DE UM PIPELINE E NÃO CORRESPONDE A UM USO COMUM DESSE RECURSO.**   \n",
    "**UM USO MAIS COMUM DE PIPELINES PODE SER VISTO NA SESSÃO [Exemplo de Exemplo de Uso com Pipelines + ColumnTransformers + Regressão Logistica](#exemplo-de-uso-com-pipelines--columntransformers--logisticregression)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb39816",
   "metadata": {},
   "source": [
    "# Exemplo de Uso com Pipelines + ColumnTransformers + LogisticRegression\n",
    "\n",
    "Esse tópico é dedicado a fornecer um exemplo mais clássico de como Pipelines e ColumnTransforms podem ser usados para realizar simplificar o processo de pré-processamento dos dados e treinar um modelo de Classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8915a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importa a classe LogisticRegression, que corresponde ao modelo que será treinado nessa sessão\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x = df.drop(columns='Comprou') # Cria uma variavel que armazena o Dataframe com os dados de ENTRADA do modelo\n",
    "y = df['Comprou'] # Cria um Dataframe que corresponde aos dados de SAÍDA DESEJADA do modelo\n",
    "\n",
    "num_cols = ['Idade', 'Salario'] # Colunas Numericas\n",
    "str_cols = ['Genero', 'Idade'] # Colunas de Texto\n",
    "\n",
    "#Criação do pipeline para o tratamento de dados numericos\n",
    "num_pipeline = Pipeline([\n",
    "    ('Imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "#Criação do Pipeline para o tratamento de dados de Texto\n",
    "str_pipeline = Pipeline([\n",
    "    ('Imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "#Junção dos pipelines com ColumnTransform\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_cols),\n",
    "    ('str', str_pipeline, str_cols)\n",
    "])\n",
    "\n",
    "#Criação do Pipeline Final -> Engloba a etapa de pré-processamento e inclui um modelo de Regressão Logistica\n",
    "model = Pipeline([\n",
    "    ('preprocessamento', preprocessing), #Realiza o pré-processamento\n",
    "    ('model', LogisticRegression()) #Modelo de Regressão Logistica\n",
    "])\n",
    "\n",
    "#Disisão dos dados em conjuto de treino e de teste\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Realização do pré-processamento dos dados e treino do modelo\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "#Avaliação do modelo\n",
    "print(f'A precisão do modelo é: {model.score(x_test, y_test) *100}%')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be7131f",
   "metadata": {},
   "source": [
    "O exemplo acima apresenta uma aplicação mais pratica e completa do uso de Pipelines e ColumnTransformers.   \n",
    "\n",
    "Embora ambos realizem um conjunto de procedimentos nos dados de entrada é possível notar que ColunTransformers só realizam esses procedimentos em colunas pré-determinadas. Além disso, um ColumnTransform pode realizar os procediemntos descritos em um Pipeline. Da mesma forma, é possível utilizar Pipeline para realizar os procedimentos descritos em um ColumnTransform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4721341",
   "metadata": {},
   "source": [
    "> O processo de Disisão dos dados em conjuto de treino e de teste é melhor detalhaodo em [train_test_split.ipynb](../Validacao/train_test_split.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371f167",
   "metadata": {},
   "source": [
    "#\n",
    "[<- Anterior](encoding.ipynb) | [Próximo ->](../Validacao/README.md)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
