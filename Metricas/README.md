[<- Anterior](../Validacao/cross_validation.ipynb) | [Próximo ->](mean_absolute_error.ipynb)

# Métricas de Avaliação

Métricas de Avaliação são indicadores utilizados para medir a qualidade e a precisão de modelos de Inteligencia Artificial. Essas métricas permitem medir o desempenho do modelo quando utiizado para realizar previsões com dados que não foram utilizados durante o treino do modelo e podem ser utilizadas não apenas para medir a eficiencia mas também para detectar problemas como overfiting e underfiting.

> NOTA: O uso de métricas de avaliação em algums casos é feito com o auxilio da função train_test_split que é mais aprofundada no tópico de [Validação](/Validacao/README.md). Para ver como funciona essa função acesse [validacao/train_test_split.ipynb](/Validacao/train_test_split.ipynb)

**TÓPICOS ABORDADOS**

- [Erro Absoluto Médio (MAE)](mean_absolute_error.ipynb)
- [Erro Quadrático Médio (MSE)](mean_squared_error.ipynb)
- [Raiz do Erro Quadrático Médio (RMSE)](root_mean_squared_error.ipynb)
- [Acurácia](accuracy.ipynb)
- [Precisão](precision.ipynb)
- [Revocação](recall.ipynb)
- [F1-Score](f1_score.ipynb)
 
---  

# Dataset de teste

Como métricas de avaliação são utilizadas para medir o desempenho de modelos de IA não serão utilizados datasets específicos para o estudo dessas métricas, sendo que, cada métrica abordada vai utilizar um dataset que melhor se encaixa no modelo que será avaliado.

Dessa forma, para cada modelo apresentado nesse diretório será utilizado um conjunto de dados diferentes. Os dados pré-definidos podem ser encontrados na pasta [Data](../Data/) desse repositório, porém, em alguns casos será utilizado um bloco de código dedicado a gerar dados que melhor se adequam ao contexto apresentado.

---

[<- Anterior](../Validacao/cross_validation.ipynb) | [Próximo ->](mean_absolute_error.ipynb)